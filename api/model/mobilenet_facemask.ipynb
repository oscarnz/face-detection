{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtV6J5D9Ckns",
        "outputId": "10bdc769-22fa-4664-f82c-6e17fa7a8f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6M5zgYZCsR0"
      },
      "outputs": [],
      "source": [
        "import os  \n",
        "import glob\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary \n",
        "\n",
        "import torch.optim as optim\n",
        "from IPython.display import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW7PKH2qEilv",
        "outputId": "54e2f8d8-9d08-4971-923d-2fa8fd6d3256"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#to check either cpu or gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT8gHrPADpPs"
      },
      "outputs": [],
      "source": [
        "#import dataset from facemask net\n",
        "!gdown --id 17UdlAXthp-SNQgk1C9dRxe0s20Z1WkLR\n",
        "!unzip '/content/Correct_Mask_128_128.zip'\n",
        "\n",
        "!gdown --id 1JPeEeclRZvOMCrSBRxa5OEgTBzoSiHGC\n",
        "!unzip '/content/thumbnails128x128-20220601T031239Z-001.zip'\n",
        "\n",
        "!gdown --id 1lKHhsrAkx0qH7YMiNr6hEDKGZN9u3iZG\n",
        "!unzip '/content/Incorrect_Mask_128_128.zip'\n",
        "\n",
        "!gdown --id 1Kowgf-mAkiqyFbgGwsTVFWCOyPRoqddP\n",
        "!unzip '/content/Incorrect_Mask1_128_128.zip'\n",
        "\n",
        "!gdown --id 16gLX3XqF46g7FnoSmRnctg7ofwY46Kh4\n",
        "!unzip '/content/Correct_Mask1_128_128.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#move all images from correct_mask1 to correct_mask\n",
        "\n",
        "file_source ='/content/Correct_Mask1_128_128'\n",
        "file_destination ='/content/Correct_Mask_128_128'\n",
        "\n",
        "for file in Path(file_source).glob('*.*'):\n",
        "    shutil.move(os.path.join(file_source,file),file_destination)\n"
      ],
      "metadata": {
        "id": "nJjwkPfcHgIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#move all images from incorrect_mask1 to incorrect_mask\n",
        "\n",
        "file_source ='/content/Incorrect_Mask1_128_128'\n",
        "file_destination ='/content/Incorrect_Mask_128_128'\n",
        "\n",
        "for file in Path(file_source).glob('*.*'):\n",
        "    shutil.move(os.path.join(file_source,file),file_destination)"
      ],
      "metadata": {
        "id": "wwzDW2OGImyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caZJVU0LDnpu"
      },
      "outputs": [],
      "source": [
        "#initialize random number \n",
        "random_seed = 124\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3eIRJe1D5PA"
      },
      "outputs": [],
      "source": [
        "#return all file paths for these labels\n",
        "#train, validation, test\n",
        "\n",
        "path = '/content'\n",
        "\n",
        "with_mask = glob.glob(os.path.join(path, \"Correct_Mask_128_128\",'*.*'))\n",
        "without_mask = glob.glob(os.path.join(path, 'thumbnails128x128','*.*'))[:54000]\n",
        "incorrect_mask = glob.glob(os.path.join(path, 'Incorrect_Mask_128_128','*.*'))\n",
        "\n",
        "images = with_mask + without_mask + incorrect_mask\n",
        "labels = np.array([1]*len(with_mask)+[0]*len(without_mask)+[2]*len(incorrect_mask))\n",
        "\n",
        "#split dataset into train, validation and test dataset\n",
        "\n",
        "images_tv, images_test, y_tv, y_test  = train_test_split(images, labels, shuffle=True, test_size=0.25, random_state=123)\n",
        "images_train, images_val, y_train, y_val  = train_test_split(images_tv, y_tv, shuffle=True, test_size=0.15, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSDRpBczD5RL",
        "outputId": "e0f9cbe0-e5a9-4cc3-dc0f-1f8f861643e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67048, 53907, 66734)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(with_mask), len(without_mask), len(incorrect_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0WNl_H5D5TR"
      },
      "outputs": [],
      "source": [
        "#transform for train dataset\n",
        "train_transforms = transforms.Compose(\n",
        "    [  \n",
        "        transforms.Resize((128,128)),\n",
        "        transforms.ColorJitter(brightness=0.25),  \n",
        "        transforms.RandomRotation(degrees=45),  \n",
        "        transforms.RandomHorizontalFlip(p=0.5),  \n",
        "       # transforms.Grayscale(), \n",
        "       # transforms.GaussianBlur(kernel_size=(7,13), sigma=(0.1, 0.2)),\n",
        "        transforms.ToTensor(), \n",
        "        #value of mean and std from the mobilenet documentation \n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "         \n",
        "        \n",
        "    ]\n",
        ")\n",
        "\n",
        "#transform for test dataset\n",
        "test_transforms = transforms.Compose([\n",
        "  #  transforms.Grayscale(), \n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzYSIrr6EIfO"
      },
      "outputs": [],
      "source": [
        "class FaceMask_Dataset(Dataset):\n",
        "    def __init__(self, img_path, img_labels, dataset_type, grayscale=True):\n",
        "        self.img_path = img_path\n",
        "        self.img_labels = torch.Tensor(img_labels)\n",
        "        self.dataset_type = dataset_type\n",
        "        #self.transforms = img_transforms \n",
        "\n",
        "        #transform train dataset\n",
        "        self.brightness_fct = transforms.ColorJitter(brightness=0.25)\n",
        "        self.rotate_fct =  transforms.RandomRotation(degrees=45) \n",
        "        self.flip_fct = transforms.RandomHorizontalFlip(p=0.5)\n",
        "        self.normalize_fct = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        self.resize_fct = transforms.Resize((128,128))\n",
        "        self.blur_fct = transforms.GaussianBlur(kernel_size=(7,13), sigma=(0.1, 0.2))\n",
        "        self.tensor_fct = transforms.ToTensor()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image\n",
        "        cur_path = self.img_path[index]\n",
        "        cur_img = PIL.Image.open(cur_path).convert('RGB')\n",
        "        #cur_img = self.transforms(cur_img)\n",
        "\n",
        "        self.resize_fct(cur_img)\n",
        "        \n",
        "        if self.dataset_type == 'Train':\n",
        "          random_num = np.random.randint(1,100)\n",
        "          if random_num <= 25:\n",
        "\n",
        "            random_num = np.random.randint(1,100)\n",
        "            if random_num <= 15:\n",
        "              cur_img = self.brightness_fct(cur_img)\n",
        "\n",
        "            random_num = np.random.randint(1,100)\n",
        "            if random_num <= 15:\n",
        "              cur_img = self.rotate_fct(cur_img)\n",
        "\n",
        "            random_num = np.random.randint(1,100)\n",
        "            if random_num <= 15:\n",
        "              cur_img = self.flip_fct(cur_img)\n",
        "            \n",
        "            random_num = np.random.randint(1,100)\n",
        "            if random_num <= 15:\n",
        "              cur_img = self.blur_fct(cur_img)\n",
        "\n",
        "        cur_img = self.tensor_fct(cur_img)\n",
        "        cur_img = self.normalize_fct(cur_img)\n",
        "        \n",
        "\n",
        "        return cur_img, self.img_labels[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " np.random.randint(1,100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn1zR5UxLB6Z",
        "outputId": "5591085d-3388-4b15-f44f-e652674c26ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5j7B6z2EIiS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "155477d34ea542bcbf5164abfb0072db",
            "bb0e784034c342079102c8274f5c2abf",
            "4247e12e2fc3438bbe2da941ff9fc3a2",
            "47a95b45a44d4854b07ee380f9f670c5",
            "d73d1bea7632496093bee5c22494a7be",
            "d467eb65ea874feabd268cb4a39859a3",
            "8a3771ff715241a48fc5ae1902b3252c",
            "212e6fdb949542e490ae25f708b288b8",
            "10aa3c90309d483e8a3a819fd15e8940",
            "b2781bae01544e66b612f98d62141354",
            "97b9a2c56f1b44e195383c4408850de8"
          ]
        },
        "outputId": "d80d612b-1d58-4384-f399-e8bc802c9bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "155477d34ea542bcbf5164abfb0072db"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "net = mobilenet_v2(pretrained=True)\n",
        "\n",
        "#freeze the layer\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#unfreeze final layer\n",
        "# net.classifier[1] = torch.nn.Linear(in_features=net.classifier[1].in_features, out_features=3)\n",
        "\n",
        "net.classifier = torch.nn.Sequential(\n",
        "          torch.nn.Dropout(p=0.2), \n",
        "          torch.nn.Linear(in_features=net.classifier[1].in_features, out_features=256),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=256, out_features=128),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=128, out_features=3)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfwdnHGheaL2",
        "outputId": "9b1f5047-7cac-494b-e8f7-aa3364fdd3ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.2, inplace=False)\n",
            "  (1): Linear(in_features=1280, out_features=256, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(net.classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msn_8Z3VeKh_",
        "outputId": "b8f8659c-36d7-4f7b-8bc8-77e47bd86e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features\n",
            "classifier\n"
          ]
        }
      ],
      "source": [
        "#identify layers in net\n",
        "for name, child in net.named_children():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw7jRXydEIk5",
        "outputId": "34b588c7-d22b-4e36-e81c-2a6b583a4493"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "MobileNetV2                                   --\n",
              "├─Sequential: 1-1                             --\n",
              "│    └─ConvNormActivation: 2-1                --\n",
              "│    │    └─Conv2d: 3-1                       (864)\n",
              "│    │    └─BatchNorm2d: 3-2                  (64)\n",
              "│    │    └─ReLU6: 3-3                        --\n",
              "│    └─InvertedResidual: 2-2                  --\n",
              "│    │    └─Sequential: 3-4                   (896)\n",
              "│    └─InvertedResidual: 2-3                  --\n",
              "│    │    └─Sequential: 3-5                   (5,136)\n",
              "│    └─InvertedResidual: 2-4                  --\n",
              "│    │    └─Sequential: 3-6                   (8,832)\n",
              "│    └─InvertedResidual: 2-5                  --\n",
              "│    │    └─Sequential: 3-7                   (10,000)\n",
              "│    └─InvertedResidual: 2-6                  --\n",
              "│    │    └─Sequential: 3-8                   (14,848)\n",
              "│    └─InvertedResidual: 2-7                  --\n",
              "│    │    └─Sequential: 3-9                   (14,848)\n",
              "│    └─InvertedResidual: 2-8                  --\n",
              "│    │    └─Sequential: 3-10                  (21,056)\n",
              "│    └─InvertedResidual: 2-9                  --\n",
              "│    │    └─Sequential: 3-11                  (54,272)\n",
              "│    └─InvertedResidual: 2-10                 --\n",
              "│    │    └─Sequential: 3-12                  (54,272)\n",
              "│    └─InvertedResidual: 2-11                 --\n",
              "│    │    └─Sequential: 3-13                  (54,272)\n",
              "│    └─InvertedResidual: 2-12                 --\n",
              "│    │    └─Sequential: 3-14                  (66,624)\n",
              "│    └─InvertedResidual: 2-13                 --\n",
              "│    │    └─Sequential: 3-15                  (118,272)\n",
              "│    └─InvertedResidual: 2-14                 --\n",
              "│    │    └─Sequential: 3-16                  (118,272)\n",
              "│    └─InvertedResidual: 2-15                 --\n",
              "│    │    └─Sequential: 3-17                  (155,264)\n",
              "│    └─InvertedResidual: 2-16                 --\n",
              "│    │    └─Sequential: 3-18                  (320,000)\n",
              "│    └─InvertedResidual: 2-17                 --\n",
              "│    │    └─Sequential: 3-19                  (320,000)\n",
              "│    └─InvertedResidual: 2-18                 --\n",
              "│    │    └─Sequential: 3-20                  (473,920)\n",
              "│    └─ConvNormActivation: 2-19               --\n",
              "│    │    └─Conv2d: 3-21                      (409,600)\n",
              "│    │    └─BatchNorm2d: 3-22                 (2,560)\n",
              "│    │    └─ReLU6: 3-23                       --\n",
              "├─Sequential: 1-2                             --\n",
              "│    └─Dropout: 2-20                          --\n",
              "│    └─Linear: 2-21                           327,936\n",
              "│    └─ReLU: 2-22                             --\n",
              "│    └─Linear: 2-23                           32,896\n",
              "│    └─ReLU: 2-24                             --\n",
              "│    └─Linear: 2-25                           387\n",
              "======================================================================\n",
              "Total params: 2,585,091\n",
              "Trainable params: 361,219\n",
              "Non-trainable params: 2,223,872\n",
              "======================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#model summary\n",
        "summary(net)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeuJkxq8EIm_"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataset, val_dataset, test_dataset, device, epochs, batch_size, l2):\n",
        "    model = model.to(device)\n",
        "\n",
        "    # construct dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # history\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    # set up loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()  \n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)  # pass in the parameters to be updated and learning rate\n",
        "     #optimizer\n",
        "    optimizer_conv = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    #scheduler = optim.lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "   \n",
        "\n",
        "    # Training Loop\n",
        "    print(\"Training Start:\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # start to train the model, activate training behavior\n",
        "\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            #print(images.size())\n",
        "            # reshape images()\n",
        "            images = images.to(device)  # reshape: from (128, 1, 28, 28) -> (128, 28 * 28) = (128, 284), move batch to device\n",
        "            labels = labels.type(torch.LongTensor).to(device)  # move to device\n",
        "\n",
        "            # forward\n",
        "            outputs = model(images)  # forward\n",
        "            pred = outputs.argmax(1)\n",
        "            cur_train_loss = criterion(outputs, labels)  # lZAss\n",
        "            cur_train_acc = (pred == labels).sum().item() / batch_size\n",
        "            \n",
        "            # backward\n",
        "            cur_train_loss.backward()   # run back propagation\n",
        "            optimizer_conv.step()            # optimizer update all model parameters\n",
        "            optimizer_conv.zero_grad()       # set gradient to zero, avoid gradient accumulating\n",
        "\n",
        "            # loss\n",
        "            train_loss += cur_train_loss \n",
        "            train_acc += cur_train_acc\n",
        "        print(epoch)\n",
        "        # valid\n",
        "        model.eval()  # start to train the model, activate training behavior\n",
        "        with torch.no_grad():  # tell pytorch not to update parameters\n",
        "            for images, labels in val_loader:\n",
        "                # calculate validation loss\n",
        "                images = images.to(device)\n",
        "                labels = labels.type(torch.LongTensor).to(device)\n",
        "                # outputs = model(images).view(-1)\n",
        "                outputs = model(images)\n",
        "\n",
        "                # loss\n",
        "                cur_valid_loss = criterion(outputs, labels)\n",
        "                val_loss += cur_valid_loss\n",
        "                # acc\n",
        "                #pred = torch.sigmoid(outputs)\n",
        "                pred = outputs.argmax(1)\n",
        "                #pred = torch.round(pred)\n",
        "                val_acc += (pred == labels).sum().item() / batch_size\n",
        "\n",
        "        # learning schedule step\n",
        "        #scheduler.step()\n",
        "\n",
        "        # print training feedback\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = train_acc / len(train_loader)\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = val_acc / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch:{epoch + 1} / {epochs}, lr: {optimizer_conv.param_groups[0]['lr']:.5f} train loss:{train_loss:.5f}, train acc: {train_acc:.5f}, valid loss:{val_loss:.5f}, valid acc:{val_acc:.5f}\")\n",
        "    \n",
        "        # update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "    \n",
        "    test_acc = 0\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4GZ1kUDEIq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688e100c-246f-40c5-bc57-3a32893a2459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Start:\n",
            "0\n",
            "Epoch:1 / 10, lr: 0.00100 train loss:0.30100, train acc: 0.88519, valid loss:0.14147, valid acc:0.94920\n",
            "1\n",
            "Epoch:2 / 10, lr: 0.00100 train loss:0.18906, train acc: 0.92474, valid loss:0.13307, valid acc:0.95076\n",
            "2\n",
            "Epoch:3 / 10, lr: 0.00100 train loss:0.17860, train acc: 0.92914, valid loss:0.12570, valid acc:0.95270\n",
            "3\n",
            "Epoch:4 / 10, lr: 0.00100 train loss:0.17248, train acc: 0.93242, valid loss:0.12054, valid acc:0.95431\n",
            "4\n",
            "Epoch:5 / 10, lr: 0.00100 train loss:0.16766, train acc: 0.93374, valid loss:0.11604, valid acc:0.95663\n",
            "5\n",
            "Epoch:6 / 10, lr: 0.00100 train loss:0.16286, train acc: 0.93500, valid loss:0.11250, valid acc:0.95843\n",
            "6\n",
            "Epoch:7 / 10, lr: 0.00100 train loss:0.15634, train acc: 0.93844, valid loss:0.10710, valid acc:0.96009\n",
            "7\n",
            "Epoch:8 / 10, lr: 0.00100 train loss:0.14964, train acc: 0.94113, valid loss:0.10491, valid acc:0.96174\n",
            "8\n",
            "Epoch:9 / 10, lr: 0.00100 train loss:0.14600, train acc: 0.94167, valid loss:0.10181, valid acc:0.96255\n",
            "9\n",
            "Epoch:10 / 10, lr: 0.00100 train loss:0.14120, train acc: 0.94428, valid loss:0.10369, valid acc:0.96009\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "train_dataset = FaceMask_Dataset(img_path=images_train, img_labels=y_train, dataset_type='Train')\n",
        "val_dataset = FaceMask_Dataset(img_path=images_val, img_labels=y_val, dataset_type='Test')\n",
        "test_dataset = FaceMask_Dataset(img_path=images_test, img_labels=y_test, dataset_type='Test')\n",
        "\n",
        "# Train the CNN model\n",
        "#cnn_model = Convnet()\n",
        "hist1 = train_model(net, train_dataset, val_dataset, test_dataset, device, batch_size=64, epochs=5, l2=0.09)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFJGIpJ8ORS_"
      },
      "outputs": [],
      "source": [
        "#save the model\n",
        "torch.save(net.state_dict(), 'pytorch_mobilenet_6.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C53ovAIPiUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e622ae04-2c2d-4478-b147-a2cabf413442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The state dict keys: \n",
            "\n",
            " odict_keys(['features.0.0.weight', 'features.0.1.weight', 'features.0.1.bias', 'features.0.1.running_mean', 'features.0.1.running_var', 'features.0.1.num_batches_tracked', 'features.1.conv.0.0.weight', 'features.1.conv.0.1.weight', 'features.1.conv.0.1.bias', 'features.1.conv.0.1.running_mean', 'features.1.conv.0.1.running_var', 'features.1.conv.0.1.num_batches_tracked', 'features.1.conv.1.weight', 'features.1.conv.2.weight', 'features.1.conv.2.bias', 'features.1.conv.2.running_mean', 'features.1.conv.2.running_var', 'features.1.conv.2.num_batches_tracked', 'features.2.conv.0.0.weight', 'features.2.conv.0.1.weight', 'features.2.conv.0.1.bias', 'features.2.conv.0.1.running_mean', 'features.2.conv.0.1.running_var', 'features.2.conv.0.1.num_batches_tracked', 'features.2.conv.1.0.weight', 'features.2.conv.1.1.weight', 'features.2.conv.1.1.bias', 'features.2.conv.1.1.running_mean', 'features.2.conv.1.1.running_var', 'features.2.conv.1.1.num_batches_tracked', 'features.2.conv.2.weight', 'features.2.conv.3.weight', 'features.2.conv.3.bias', 'features.2.conv.3.running_mean', 'features.2.conv.3.running_var', 'features.2.conv.3.num_batches_tracked', 'features.3.conv.0.0.weight', 'features.3.conv.0.1.weight', 'features.3.conv.0.1.bias', 'features.3.conv.0.1.running_mean', 'features.3.conv.0.1.running_var', 'features.3.conv.0.1.num_batches_tracked', 'features.3.conv.1.0.weight', 'features.3.conv.1.1.weight', 'features.3.conv.1.1.bias', 'features.3.conv.1.1.running_mean', 'features.3.conv.1.1.running_var', 'features.3.conv.1.1.num_batches_tracked', 'features.3.conv.2.weight', 'features.3.conv.3.weight', 'features.3.conv.3.bias', 'features.3.conv.3.running_mean', 'features.3.conv.3.running_var', 'features.3.conv.3.num_batches_tracked', 'features.4.conv.0.0.weight', 'features.4.conv.0.1.weight', 'features.4.conv.0.1.bias', 'features.4.conv.0.1.running_mean', 'features.4.conv.0.1.running_var', 'features.4.conv.0.1.num_batches_tracked', 'features.4.conv.1.0.weight', 'features.4.conv.1.1.weight', 'features.4.conv.1.1.bias', 'features.4.conv.1.1.running_mean', 'features.4.conv.1.1.running_var', 'features.4.conv.1.1.num_batches_tracked', 'features.4.conv.2.weight', 'features.4.conv.3.weight', 'features.4.conv.3.bias', 'features.4.conv.3.running_mean', 'features.4.conv.3.running_var', 'features.4.conv.3.num_batches_tracked', 'features.5.conv.0.0.weight', 'features.5.conv.0.1.weight', 'features.5.conv.0.1.bias', 'features.5.conv.0.1.running_mean', 'features.5.conv.0.1.running_var', 'features.5.conv.0.1.num_batches_tracked', 'features.5.conv.1.0.weight', 'features.5.conv.1.1.weight', 'features.5.conv.1.1.bias', 'features.5.conv.1.1.running_mean', 'features.5.conv.1.1.running_var', 'features.5.conv.1.1.num_batches_tracked', 'features.5.conv.2.weight', 'features.5.conv.3.weight', 'features.5.conv.3.bias', 'features.5.conv.3.running_mean', 'features.5.conv.3.running_var', 'features.5.conv.3.num_batches_tracked', 'features.6.conv.0.0.weight', 'features.6.conv.0.1.weight', 'features.6.conv.0.1.bias', 'features.6.conv.0.1.running_mean', 'features.6.conv.0.1.running_var', 'features.6.conv.0.1.num_batches_tracked', 'features.6.conv.1.0.weight', 'features.6.conv.1.1.weight', 'features.6.conv.1.1.bias', 'features.6.conv.1.1.running_mean', 'features.6.conv.1.1.running_var', 'features.6.conv.1.1.num_batches_tracked', 'features.6.conv.2.weight', 'features.6.conv.3.weight', 'features.6.conv.3.bias', 'features.6.conv.3.running_mean', 'features.6.conv.3.running_var', 'features.6.conv.3.num_batches_tracked', 'features.7.conv.0.0.weight', 'features.7.conv.0.1.weight', 'features.7.conv.0.1.bias', 'features.7.conv.0.1.running_mean', 'features.7.conv.0.1.running_var', 'features.7.conv.0.1.num_batches_tracked', 'features.7.conv.1.0.weight', 'features.7.conv.1.1.weight', 'features.7.conv.1.1.bias', 'features.7.conv.1.1.running_mean', 'features.7.conv.1.1.running_var', 'features.7.conv.1.1.num_batches_tracked', 'features.7.conv.2.weight', 'features.7.conv.3.weight', 'features.7.conv.3.bias', 'features.7.conv.3.running_mean', 'features.7.conv.3.running_var', 'features.7.conv.3.num_batches_tracked', 'features.8.conv.0.0.weight', 'features.8.conv.0.1.weight', 'features.8.conv.0.1.bias', 'features.8.conv.0.1.running_mean', 'features.8.conv.0.1.running_var', 'features.8.conv.0.1.num_batches_tracked', 'features.8.conv.1.0.weight', 'features.8.conv.1.1.weight', 'features.8.conv.1.1.bias', 'features.8.conv.1.1.running_mean', 'features.8.conv.1.1.running_var', 'features.8.conv.1.1.num_batches_tracked', 'features.8.conv.2.weight', 'features.8.conv.3.weight', 'features.8.conv.3.bias', 'features.8.conv.3.running_mean', 'features.8.conv.3.running_var', 'features.8.conv.3.num_batches_tracked', 'features.9.conv.0.0.weight', 'features.9.conv.0.1.weight', 'features.9.conv.0.1.bias', 'features.9.conv.0.1.running_mean', 'features.9.conv.0.1.running_var', 'features.9.conv.0.1.num_batches_tracked', 'features.9.conv.1.0.weight', 'features.9.conv.1.1.weight', 'features.9.conv.1.1.bias', 'features.9.conv.1.1.running_mean', 'features.9.conv.1.1.running_var', 'features.9.conv.1.1.num_batches_tracked', 'features.9.conv.2.weight', 'features.9.conv.3.weight', 'features.9.conv.3.bias', 'features.9.conv.3.running_mean', 'features.9.conv.3.running_var', 'features.9.conv.3.num_batches_tracked', 'features.10.conv.0.0.weight', 'features.10.conv.0.1.weight', 'features.10.conv.0.1.bias', 'features.10.conv.0.1.running_mean', 'features.10.conv.0.1.running_var', 'features.10.conv.0.1.num_batches_tracked', 'features.10.conv.1.0.weight', 'features.10.conv.1.1.weight', 'features.10.conv.1.1.bias', 'features.10.conv.1.1.running_mean', 'features.10.conv.1.1.running_var', 'features.10.conv.1.1.num_batches_tracked', 'features.10.conv.2.weight', 'features.10.conv.3.weight', 'features.10.conv.3.bias', 'features.10.conv.3.running_mean', 'features.10.conv.3.running_var', 'features.10.conv.3.num_batches_tracked', 'features.11.conv.0.0.weight', 'features.11.conv.0.1.weight', 'features.11.conv.0.1.bias', 'features.11.conv.0.1.running_mean', 'features.11.conv.0.1.running_var', 'features.11.conv.0.1.num_batches_tracked', 'features.11.conv.1.0.weight', 'features.11.conv.1.1.weight', 'features.11.conv.1.1.bias', 'features.11.conv.1.1.running_mean', 'features.11.conv.1.1.running_var', 'features.11.conv.1.1.num_batches_tracked', 'features.11.conv.2.weight', 'features.11.conv.3.weight', 'features.11.conv.3.bias', 'features.11.conv.3.running_mean', 'features.11.conv.3.running_var', 'features.11.conv.3.num_batches_tracked', 'features.12.conv.0.0.weight', 'features.12.conv.0.1.weight', 'features.12.conv.0.1.bias', 'features.12.conv.0.1.running_mean', 'features.12.conv.0.1.running_var', 'features.12.conv.0.1.num_batches_tracked', 'features.12.conv.1.0.weight', 'features.12.conv.1.1.weight', 'features.12.conv.1.1.bias', 'features.12.conv.1.1.running_mean', 'features.12.conv.1.1.running_var', 'features.12.conv.1.1.num_batches_tracked', 'features.12.conv.2.weight', 'features.12.conv.3.weight', 'features.12.conv.3.bias', 'features.12.conv.3.running_mean', 'features.12.conv.3.running_var', 'features.12.conv.3.num_batches_tracked', 'features.13.conv.0.0.weight', 'features.13.conv.0.1.weight', 'features.13.conv.0.1.bias', 'features.13.conv.0.1.running_mean', 'features.13.conv.0.1.running_var', 'features.13.conv.0.1.num_batches_tracked', 'features.13.conv.1.0.weight', 'features.13.conv.1.1.weight', 'features.13.conv.1.1.bias', 'features.13.conv.1.1.running_mean', 'features.13.conv.1.1.running_var', 'features.13.conv.1.1.num_batches_tracked', 'features.13.conv.2.weight', 'features.13.conv.3.weight', 'features.13.conv.3.bias', 'features.13.conv.3.running_mean', 'features.13.conv.3.running_var', 'features.13.conv.3.num_batches_tracked', 'features.14.conv.0.0.weight', 'features.14.conv.0.1.weight', 'features.14.conv.0.1.bias', 'features.14.conv.0.1.running_mean', 'features.14.conv.0.1.running_var', 'features.14.conv.0.1.num_batches_tracked', 'features.14.conv.1.0.weight', 'features.14.conv.1.1.weight', 'features.14.conv.1.1.bias', 'features.14.conv.1.1.running_mean', 'features.14.conv.1.1.running_var', 'features.14.conv.1.1.num_batches_tracked', 'features.14.conv.2.weight', 'features.14.conv.3.weight', 'features.14.conv.3.bias', 'features.14.conv.3.running_mean', 'features.14.conv.3.running_var', 'features.14.conv.3.num_batches_tracked', 'features.15.conv.0.0.weight', 'features.15.conv.0.1.weight', 'features.15.conv.0.1.bias', 'features.15.conv.0.1.running_mean', 'features.15.conv.0.1.running_var', 'features.15.conv.0.1.num_batches_tracked', 'features.15.conv.1.0.weight', 'features.15.conv.1.1.weight', 'features.15.conv.1.1.bias', 'features.15.conv.1.1.running_mean', 'features.15.conv.1.1.running_var', 'features.15.conv.1.1.num_batches_tracked', 'features.15.conv.2.weight', 'features.15.conv.3.weight', 'features.15.conv.3.bias', 'features.15.conv.3.running_mean', 'features.15.conv.3.running_var', 'features.15.conv.3.num_batches_tracked', 'features.16.conv.0.0.weight', 'features.16.conv.0.1.weight', 'features.16.conv.0.1.bias', 'features.16.conv.0.1.running_mean', 'features.16.conv.0.1.running_var', 'features.16.conv.0.1.num_batches_tracked', 'features.16.conv.1.0.weight', 'features.16.conv.1.1.weight', 'features.16.conv.1.1.bias', 'features.16.conv.1.1.running_mean', 'features.16.conv.1.1.running_var', 'features.16.conv.1.1.num_batches_tracked', 'features.16.conv.2.weight', 'features.16.conv.3.weight', 'features.16.conv.3.bias', 'features.16.conv.3.running_mean', 'features.16.conv.3.running_var', 'features.16.conv.3.num_batches_tracked', 'features.17.conv.0.0.weight', 'features.17.conv.0.1.weight', 'features.17.conv.0.1.bias', 'features.17.conv.0.1.running_mean', 'features.17.conv.0.1.running_var', 'features.17.conv.0.1.num_batches_tracked', 'features.17.conv.1.0.weight', 'features.17.conv.1.1.weight', 'features.17.conv.1.1.bias', 'features.17.conv.1.1.running_mean', 'features.17.conv.1.1.running_var', 'features.17.conv.1.1.num_batches_tracked', 'features.17.conv.2.weight', 'features.17.conv.3.weight', 'features.17.conv.3.bias', 'features.17.conv.3.running_mean', 'features.17.conv.3.running_var', 'features.17.conv.3.num_batches_tracked', 'features.18.0.weight', 'features.18.1.weight', 'features.18.1.bias', 'features.18.1.running_mean', 'features.18.1.running_var', 'features.18.1.num_batches_tracked', 'classifier.1.weight', 'classifier.1.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n"
          ]
        }
      ],
      "source": [
        "#load the model\n",
        "state_dict = torch.load('/content/pytorch_mobilenet_6.pth')\n",
        "print(\"The state dict keys: \\n\\n\", net.state_dict().keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model with test dataset\n",
        "\n",
        "import torch  \n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "PATH1 = \"/content/pytorch_mobilenet_5.pth\"\n",
        "model = net.to(device)\n",
        "\n",
        "\n",
        "def test_model(PATH1, val_dataset, device, test_acc):\n",
        "    \n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "    model.eval()\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    probs = []\n",
        "    test_acc = 0\n",
        "    with torch.no_grad():\n",
        "      for (x, y) in test_loader:\n",
        "          \n",
        "        x = x.to(device)\n",
        "        #print(x.size())\n",
        "        y = y.to(device)\n",
        "        y_pred = model(x)   \n",
        "        y_prob = y_pred.argmax(1)\n",
        "        #y_prob = torch.round(y_prob)\n",
        "\n",
        "        # images.append(x.to(device))\n",
        "        labels.append(y.to(device))\n",
        "        probs.append(y_prob.to(device))\n",
        "  \n",
        "        test_acc += (y_prob == y).sum().item()\n",
        "\n",
        "    # images = torch.cat(images, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    probs = torch.cat(probs, dim=0)\n",
        "\n",
        "    print(f'Test Accuracy: {(test_acc/len(test_loader))}')\n",
        "\n",
        "    return probs, labels\n",
        "    \n",
        "#confusion matrix\n",
        "def plot_confusion_matrix(labels, pred, classes):\n",
        "  \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    cm = confusion_matrix(labels, pred)\n",
        "    cm = ConfusionMatrixDisplay(cm)\n",
        "    cm.plot(values_format='d', cmap='Blues', ax=ax)\n",
        "    ax.xaxis.set_ticklabels(['without mask', 'with mask', 'incorrect mask']); ax.yaxis.set_ticklabels(['without mask', 'with mask', 'incorrect mask']);\n",
        "    plt.xticks(rotation=2)\n",
        "\n",
        "#images, labels, probs = test_model(model, test_dataset, device, test_acc=0)\n",
        "probs, labels = test_model(model, test_dataset, device, test_acc=0)\n",
        "#pred = probs.argmax(-1)\n",
        "plot_confusion_matrix(labels, probs, 3)"
      ],
      "metadata": {
        "id": "dRSY0wKKRYmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z5f2f6lTojz"
      },
      "outputs": [],
      "source": [
        "#test the model with google images\n",
        "\n",
        "from PIL import Image \n",
        "import torch\n",
        "\n",
        "PATH = \"/content/pytorch_mobilenet_5.pth\"\n",
        "\n",
        "maskclasses = [\"without mask\", \"with mask\", \"incorrect_mask\"]\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "model = mobilenet_v2()\n",
        "#model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=3)\n",
        "\n",
        "model.classifier = torch.nn.Sequential(\n",
        "          torch.nn.Dropout(p=0.2), \n",
        "          torch.nn.Linear(in_features=net.classifier[1].in_features, out_features=256),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=256, out_features=128),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=128, out_features=3)\n",
        "        )\n",
        "\n",
        "model.load_state_dict(torch.load(PATH, map_location=device))\n",
        "\n",
        "\n",
        "def classify(model, image_path, maskclasses):\n",
        "  print()\n",
        "  model.eval()\n",
        "  image = PIL.Image.open(image_path)\n",
        "  image_tensor = test_transforms(image)\n",
        "  print(image)\n",
        "  #print(image_tensor)\n",
        "  image_tensor = image_tensor.unsqueeze(0)\n",
        "  output = model(image_tensor)\n",
        "\n",
        "  print(maskclasses[output.argmax(1)])\n",
        "  print(output)\n",
        "  print(image_tensor.size())\n",
        "  \n",
        "classify(model, \"/content/with_mask1.JPG\", maskclasses)\n",
        "#classify(model, \"/content/no_mask1.JPG\", maskclasses)\n",
        "#classify(model, \"/content/g1_incorrectmask.JPG\", maskclasses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_detection"
      ],
      "metadata": {
        "id": "wIEhfH-GY2uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model with google images\n",
        "\n",
        "from PIL import Image \n",
        "import torch\n",
        "import face_detection\n",
        "\n",
        "PATH = \"/content/pytorch_mobilenet_5.pth\"\n",
        "\n",
        "\n",
        "maskclasses = [\"without mask\", \"with mask\", \"incorrect_mask\"]\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "model = mobilenet_v2()\n",
        "#model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=3)\n",
        "\n",
        "model.classifier = torch.nn.Sequential(\n",
        "          torch.nn.Dropout(p=0.2), \n",
        "          torch.nn.Linear(in_features=net.classifier[1].in_features, out_features=256),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=256, out_features=128),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(in_features=128, out_features=3)\n",
        "        )\n",
        "\n",
        "model.load_state_dict(torch.load(PATH, map_location=device))\n",
        "\n",
        "\n",
        "def classify(model, image_path, maskclasses):\n",
        "  print()\n",
        "  model.eval()\n",
        "\n",
        "  image = PIL.Image.open(image_path).convert('RGB')\n",
        "  image = np.uint8(image)\n",
        "  detector = face_detection.build_detector(\"RetinaNetMobileNetV1\", confidence_threshold=.65,nms_iou_threshold=.3)\n",
        "  bounding_boxes = detector.detect(np.array(image))\n",
        "  print(image[0,0,:])\n",
        "  for i in bounding_boxes[:, :4]:\n",
        "    \n",
        "    x0, y0, x1, y1 = [int(_) for _ in i]\n",
        "\n",
        "    face_to_analyze = image[max(11, y0)-10:y0+y1+10, max(11, x0)-10:x0+x1+10]\n",
        "    face_to_analyze = Image.fromarray(np.uint8(face_to_analyze))\n",
        "    face_prepared = test_transforms(face_to_analyze)\n",
        "    image_tensor = face_prepared.unsqueeze(0)\n",
        "    output = model(image_tensor)\n",
        "\n",
        "    pred = output.argmax(1) \n",
        "\n",
        "    if pred == 0:\n",
        "      label = \"Without Mask\"\n",
        "    elif pred == 1:\n",
        "      label = \"Mask\"\n",
        "    else:\n",
        "      label = \"Incorrect Mask\"\n",
        "  \n",
        "  # image_tensor = test_transforms(image)\n",
        "  # print(image)\n",
        "  # #print(image_tensor)\n",
        "  # image_tensor = image_tensor.unsqueeze(0)\n",
        "  # output = model(image_tensor)\n",
        "\n",
        "  # print(maskclasses[output.argmax(1)])\n",
        "  # print(output)\n",
        "  # print(image_tensor[0,0,:])\n",
        "  # print(image_tensor.size())\n",
        "  \n",
        "classify(model, \"/content/g2_withmask.JPG\", maskclasses)\n",
        "classify(model, \"/content/g1_nomask.jpg\", maskclasses)\n",
        "classify(model, \"/content/g1_incorrectmask.JPG\", maskclasses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svg_1GJeYq7p",
        "outputId": "76d7286b-e8e4-40d5-9a80-a824680583f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[186 172 169]\n",
            "\n",
            "[26 23 16]\n",
            "\n",
            "[202 205 196]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "xvE2zTfNd7L2",
        "outputId": "747c1739-c67e-49e3-b222-5f773900aeb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6aa307a1455d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'image_tensor' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "mobilenet_facemask.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "155477d34ea542bcbf5164abfb0072db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb0e784034c342079102c8274f5c2abf",
              "IPY_MODEL_4247e12e2fc3438bbe2da941ff9fc3a2",
              "IPY_MODEL_47a95b45a44d4854b07ee380f9f670c5"
            ],
            "layout": "IPY_MODEL_d73d1bea7632496093bee5c22494a7be"
          }
        },
        "bb0e784034c342079102c8274f5c2abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d467eb65ea874feabd268cb4a39859a3",
            "placeholder": "​",
            "style": "IPY_MODEL_8a3771ff715241a48fc5ae1902b3252c",
            "value": "100%"
          }
        },
        "4247e12e2fc3438bbe2da941ff9fc3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_212e6fdb949542e490ae25f708b288b8",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10aa3c90309d483e8a3a819fd15e8940",
            "value": 14212972
          }
        },
        "47a95b45a44d4854b07ee380f9f670c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2781bae01544e66b612f98d62141354",
            "placeholder": "​",
            "style": "IPY_MODEL_97b9a2c56f1b44e195383c4408850de8",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 14.8MB/s]"
          }
        },
        "d73d1bea7632496093bee5c22494a7be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d467eb65ea874feabd268cb4a39859a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a3771ff715241a48fc5ae1902b3252c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "212e6fdb949542e490ae25f708b288b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10aa3c90309d483e8a3a819fd15e8940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2781bae01544e66b612f98d62141354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b9a2c56f1b44e195383c4408850de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}